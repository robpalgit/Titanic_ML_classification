{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Family_cnt</th>\n",
       "      <th>Cabin_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age     Fare  Family_cnt  Cabin_ind\n",
       "0         0       3    0  22.0   7.2500           1          0\n",
       "1         1       1    1  38.0  71.2833           1          1\n",
       "2         1       3    1  26.0   7.9250           0          0\n",
       "3         1       1    1  35.0  53.1000           1          1\n",
       "4         0       3    0  35.0   8.0500           0          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "df['Family_cnt'] = df['SibSp'] + df['Parch']\n",
    "\n",
    "df['Cabin_ind'] = np.where(df['Cabin'].isnull(), 0, 1)\n",
    "\n",
    "gender_num = {'male': 0, 'female': 1}\n",
    "df['Sex'] = df['Sex'].map(gender_num)\n",
    "\n",
    "df.drop(['SibSp', 'Parch', 'Cabin', 'Embarked', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validation / Test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "features = df.drop('Survived', axis=1)\n",
    "labels = df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "for dataset in [y_train, y_val, y_test]:\n",
    "    print(round(len(dataset) / len(labels), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_score(hp_optimizer):\n",
    "    print('BEST SCORE: {} - PARAMS: {}\\n'.format(round(hp_optimizer.best_score_, 3), hp_optimizer.best_params_))\n",
    "\n",
    "    means = hp_optimizer.cv_results_['mean_test_score']\n",
    "    stds = hp_optimizer.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, hp_optimizer.cv_results_['params']):\n",
    "        print('Score: {} (+/-{}) for Params: {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SCORE: 0.798 - PARAMS: {'C': 1}\n",
      "\n",
      "Score: 0.678 (+/-0.092) for Params: {'C': 0.001}\n",
      "Score: 0.704 (+/-0.099) for Params: {'C': 0.01}\n",
      "Score: 0.796 (+/-0.13) for Params: {'C': 0.1}\n",
      "Score: 0.798 (+/-0.123) for Params: {'C': 1}\n",
      "Score: 0.794 (+/-0.118) for Params: {'C': 10}\n",
      "Score: 0.794 (+/-0.118) for Params: {'C': 100}\n",
      "Score: 0.794 (+/-0.118) for Params: {'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "CV_1 = GridSearchCV(lr, parameters, cv=5, iid=False)\n",
    "CV_1.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_best_score(CV_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = CV_1.best_estimator_\n",
    "LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SCORE: 0.796 - PARAMS: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Score: 0.796 (+/-0.116) for Params: {'C': 0.1, 'kernel': 'linear'}\n",
      "Score: 0.654 (+/-0.062) for Params: {'C': 0.1, 'kernel': 'rbf'}\n",
      "Score: 0.796 (+/-0.116) for Params: {'C': 1, 'kernel': 'linear'}\n",
      "Score: 0.661 (+/-0.05) for Params: {'C': 1, 'kernel': 'rbf'}\n",
      "Score: 0.796 (+/-0.116) for Params: {'C': 100, 'kernel': 'linear'}\n",
      "Score: 0.788 (+/-0.113) for Params: {'C': 100, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(gamma='scale')\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 100]\n",
    "}\n",
    "\n",
    "CV_2 = GridSearchCV(svc, parameters, cv=5, iid=False)\n",
    "CV_2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_best_score(CV_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = CV_2.best_estimator_\n",
    "SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SCORE: 0.809 - PARAMS: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "\n",
      "Score: 0.639 (+/-0.125) for Params: {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "Score: 0.621 (+/-0.268) for Params: {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.543 (+/-0.341) for Params: {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.569 (+/-0.409) for Params: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "Score: 0.598 (+/-0.289) for Params: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.558 (+/-0.428) for Params: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.62 (+/-0.017) for Params: {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "Score: 0.62 (+/-0.259) for Params: {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.676 (+/-0.183) for Params: {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.788 (+/-0.155) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "Score: 0.766 (+/-0.064) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.792 (+/-0.136) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.809 (+/-0.088) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "Score: 0.807 (+/-0.089) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.803 (+/-0.101) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.798 (+/-0.084) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "Score: 0.784 (+/-0.113) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.803 (+/-0.106) for Params: {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.798 (+/-0.076) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "Score: 0.805 (+/-0.105) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.786 (+/-0.144) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.794 (+/-0.089) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "Score: 0.792 (+/-0.074) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.803 (+/-0.083) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "Score: 0.775 (+/-0.065) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "Score: 0.785 (+/-0.106) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "Score: 0.775 (+/-0.064) for Params: {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs')\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "CV_3 = GridSearchCV(mlp, parameters, cv=5, iid=False)\n",
    "CV_3.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_best_score(CV_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = CV_3.best_estimator_\n",
    "MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SCORE: 0.828 - PARAMS: {'max_depth': 4, 'n_estimators': 250}\n",
      "\n",
      "Score: 0.796 (+/-0.116) for Params: {'max_depth': 2, 'n_estimators': 5}\n",
      "Score: 0.801 (+/-0.12) for Params: {'max_depth': 2, 'n_estimators': 50}\n",
      "Score: 0.798 (+/-0.13) for Params: {'max_depth': 2, 'n_estimators': 250}\n",
      "Score: 0.814 (+/-0.105) for Params: {'max_depth': 4, 'n_estimators': 5}\n",
      "Score: 0.824 (+/-0.103) for Params: {'max_depth': 4, 'n_estimators': 50}\n",
      "Score: 0.828 (+/-0.104) for Params: {'max_depth': 4, 'n_estimators': 250}\n",
      "Score: 0.803 (+/-0.073) for Params: {'max_depth': 8, 'n_estimators': 5}\n",
      "Score: 0.822 (+/-0.065) for Params: {'max_depth': 8, 'n_estimators': 50}\n",
      "Score: 0.816 (+/-0.064) for Params: {'max_depth': 8, 'n_estimators': 250}\n",
      "Score: 0.801 (+/-0.05) for Params: {'max_depth': 16, 'n_estimators': 5}\n",
      "Score: 0.801 (+/-0.04) for Params: {'max_depth': 16, 'n_estimators': 50}\n",
      "Score: 0.816 (+/-0.028) for Params: {'max_depth': 16, 'n_estimators': 250}\n",
      "Score: 0.79 (+/-0.059) for Params: {'max_depth': 32, 'n_estimators': 5}\n",
      "Score: 0.809 (+/-0.036) for Params: {'max_depth': 32, 'n_estimators': 50}\n",
      "Score: 0.813 (+/-0.032) for Params: {'max_depth': 32, 'n_estimators': 250}\n",
      "Score: 0.786 (+/-0.062) for Params: {'max_depth': None, 'n_estimators': 5}\n",
      "Score: 0.811 (+/-0.021) for Params: {'max_depth': None, 'n_estimators': 50}\n",
      "Score: 0.815 (+/-0.037) for Params: {'max_depth': None, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250],\n",
    "    'max_depth': [2, 4, 8, 16, 32, None]\n",
    "}\n",
    "\n",
    "CV_4 = GridSearchCV(rf, parameters, cv=5, iid=False)\n",
    "CV_4.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_best_score(CV_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = CV_4.best_estimator_\n",
    "RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SCORE: 0.841 - PARAMS: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "\n",
      "Score: 0.624 (+/-0.005) for Params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "Score: 0.796 (+/-0.116) for Params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "Score: 0.796 (+/-0.116) for Params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "Score: 0.811 (+/-0.118) for Params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "Score: 0.624 (+/-0.005) for Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "Score: 0.811 (+/-0.071) for Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "Score: 0.829 (+/-0.076) for Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "Score: 0.841 (+/-0.079) for Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "Score: 0.624 (+/-0.005) for Params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "Score: 0.82 (+/-0.052) for Params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "Score: 0.818 (+/-0.045) for Params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "Score: 0.831 (+/-0.043) for Params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
      "Score: 0.624 (+/-0.005) for Params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "Score: 0.818 (+/-0.054) for Params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "Score: 0.816 (+/-0.035) for Params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "Score: 0.805 (+/-0.019) for Params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}\n",
      "Score: 0.624 (+/-0.005) for Params: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}\n",
      "Score: 0.801 (+/-0.055) for Params: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}\n",
      "Score: 0.807 (+/-0.019) for Params: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}\n",
      "Score: 0.783 (+/-0.033) for Params: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}\n",
      "Score: 0.796 (+/-0.116) for Params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "Score: 0.815 (+/-0.119) for Params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Score: 0.818 (+/-0.112) for Params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "Score: 0.828 (+/-0.093) for Params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "Score: 0.813 (+/-0.074) for Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "Score: 0.837 (+/-0.077) for Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Score: 0.83 (+/-0.04) for Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "Score: 0.811 (+/-0.03) for Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Score: 0.813 (+/-0.053) for Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "Score: 0.816 (+/-0.009) for Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "Score: 0.803 (+/-0.056) for Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "Score: 0.805 (+/-0.047) for Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "Score: 0.82 (+/-0.053) for Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "Score: 0.8 (+/-0.036) for Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "Score: 0.8 (+/-0.046) for Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "Score: 0.796 (+/-0.039) for Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n",
      "Score: 0.802 (+/-0.052) for Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}\n",
      "Score: 0.788 (+/-0.043) for Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}\n",
      "Score: 0.796 (+/-0.044) for Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}\n",
      "Score: 0.798 (+/-0.047) for Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "Score: 0.818 (+/-0.1) for Params: {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "Score: 0.83 (+/-0.078) for Params: {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Score: 0.828 (+/-0.069) for Params: {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "Score: 0.818 (+/-0.082) for Params: {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}\n",
      "Score: 0.82 (+/-0.063) for Params: {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "Score: 0.798 (+/-0.026) for Params: {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Score: 0.802 (+/-0.046) for Params: {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "Score: 0.803 (+/-0.043) for Params: {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Score: 0.811 (+/-0.047) for Params: {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "Score: 0.794 (+/-0.04) for Params: {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "Score: 0.802 (+/-0.085) for Params: {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "Score: 0.807 (+/-0.082) for Params: {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}\n",
      "Score: 0.798 (+/-0.055) for Params: {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "Score: 0.794 (+/-0.052) for Params: {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "Score: 0.813 (+/-0.054) for Params: {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "Score: 0.792 (+/-0.055) for Params: {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}\n",
      "Score: 0.772 (+/-0.02) for Params: {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}\n",
      "Score: 0.8 (+/-0.046) for Params: {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}\n",
      "Score: 0.802 (+/-0.035) for Params: {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}\n",
      "Score: 0.796 (+/-0.042) for Params: {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}\n",
      "Score: 0.204 (+/-0.116) for Params: {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "Score: 0.204 (+/-0.116) for Params: {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "Score: 0.204 (+/-0.116) for Params: {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "Score: 0.204 (+/-0.116) for Params: {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}\n",
      "Score: 0.31 (+/-0.192) for Params: {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "Score: 0.31 (+/-0.192) for Params: {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "Score: 0.31 (+/-0.192) for Params: {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "Score: 0.31 (+/-0.192) for Params: {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}\n",
      "Score: 0.55 (+/-0.362) for Params: {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "Score: 0.441 (+/-0.313) for Params: {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "Score: 0.503 (+/-0.37) for Params: {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "Score: 0.503 (+/-0.37) for Params: {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}\n",
      "Score: 0.612 (+/-0.167) for Params: {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "Score: 0.583 (+/-0.183) for Params: {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "Score: 0.588 (+/-0.199) for Params: {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "Score: 0.601 (+/-0.192) for Params: {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}\n",
      "Score: 0.711 (+/-0.139) for Params: {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}\n",
      "Score: 0.713 (+/-0.119) for Params: {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}\n",
      "Score: 0.713 (+/-0.122) for Params: {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}\n",
      "Score: 0.715 (+/-0.135) for Params: {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}\n",
      "Score: 0.376 (+/-0.005) for Params: {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}\n",
      "Score: 0.376 (+/-0.005) for Params: {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}\n",
      "Score: 0.376 (+/-0.005) for Params: {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}\n",
      "Score: 0.376 (+/-0.005) for Params: {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}\n",
      "Score: 0.29 (+/-0.104) for Params: {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}\n",
      "Score: 0.29 (+/-0.104) for Params: {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}\n",
      "Score: 0.29 (+/-0.104) for Params: {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}\n",
      "Score: 0.29 (+/-0.104) for Params: {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}\n",
      "Score: 0.376 (+/-0.179) for Params: {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}\n",
      "Score: 0.372 (+/-0.17) for Params: {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}\n",
      "Score: 0.372 (+/-0.183) for Params: {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}\n",
      "Score: 0.374 (+/-0.176) for Params: {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}\n",
      "Score: 0.556 (+/-0.134) for Params: {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}\n",
      "Score: 0.581 (+/-0.106) for Params: {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}\n",
      "Score: 0.569 (+/-0.102) for Params: {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}\n",
      "Score: 0.562 (+/-0.108) for Params: {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}\n",
      "Score: 0.668 (+/-0.101) for Params: {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}\n",
      "Score: 0.693 (+/-0.092) for Params: {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}\n",
      "Score: 0.663 (+/-0.131) for Params: {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}\n",
      "Score: 0.663 (+/-0.061) for Params: {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "CV_5 = GridSearchCV(gb, parameters, cv=5, iid=False)\n",
    "CV_5.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_best_score(CV_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = CV_5.best_estimator_\n",
    "GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best models evaluation on validation dataset and final model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_names = ['LR', 'SVM', 'MLP', 'RF', 'GB']\n",
    "mdl_list = [LR, SVM, MLP, RF, GB]\n",
    "models = dict(zip(mdl_names, mdl_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.77 / Precision: 0.707 / Recall: 0.631 / Latency: 0.0ms\n",
      "SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 15.6ms\n",
      "MLP -- Accuracy: 0.787 / Precision: 0.8 / Recall: 0.554 / Latency: 15.6ms\n",
      "RF -- Accuracy: 0.815 / Precision: 0.82 / Recall: 0.631 / Latency: 62.5ms\n",
      "GB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 0.0ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 15.6ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Gradient Boosting', models['GB'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
